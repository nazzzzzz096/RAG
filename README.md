# ğŸ§  Legal Document AI Assistant

This project provides two core functionalities powered by local language models and LangChain:

1. **Retrieval-Augmented Generation (RAG)** â€” Ask questions and get answers based on a legal PDF.
2. **Summarization** â€” Generate concise summaries of legal documents.

---

## ğŸ“ Project Structure
project/
â”‚
â”œâ”€â”€ app/
â”‚ â”œâ”€â”€ ingest.py # Ingest PDF and create FAISS index
â”‚ â”œâ”€â”€ rag_chat.py # Ask questions using RAG pipeline
â”‚ â”œâ”€â”€ summarize.py # Summarize legal documents
â”‚
â”œâ”€â”€ vectorstore/
â”‚ â””â”€â”€ legal_index/ # FAISS index files (auto-generated)
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ case.pdf # Sample legal PDF document
â”‚
â”œâ”€â”€ flan-t5-base/ # Local fine-tuned FLAN-T5 model (for RAG)
â”œâ”€â”€ flan-t5-small/ # Local fine-tuned FLAN-T5 model (for summarization)
â””â”€â”€ README.md # You're reading it!

ğŸ› ï¸ Technologies and Tools Used
ğŸ§  LangChain
   Used for building the RAG pipeline and summarization logic:

   RetrievalQA for retrieval-augmented question answering

   load_summarize_chain for the map-reduce summarization strategy

   Prompt Template for custom prompt formatting

   RecursiveCharacterTextSplitter for splitting documents into chunks

   PyPDFLoader for reading PDF files

ğŸ¤— Hugging Face Transformers
   Used for loading and running local text-to-text models:

   AutoTokenizer, AutoModelForSeq2SeqLM: Load pre-trained T5 models (FLAN-T5 base/small)

   pipeline: Set up text2text-generation pipeline for both answering and summarizing

ğŸ§¬ Hugging Face Sentence Transformers
  Used to generate dense vector embeddings for document chunks:

  Model: sentence-transformers/all-MiniLM-L6-v2 â€” small, fast, and efficient embedding model

ğŸ“¦ FAISS (Facebook AI Similarity Search)
  Used as the vector store backend to store and search over embeddings for document chunks:

  Enables efficient retrieval of top-k similar chunks based on a question

ğŸ“‚ Local Models
   All models are downloaded and run locally (no API calls or internet required):

   flan-t5-base for answering questions (QA)

   flan-t5-small for document summarization

ğŸ”„ Workflows & Concepts
  ğŸ” Retrieval-Augmented Generation (RAG)
     Combines retrieval (from FAISS vector store) and generation (via FLAN-T5) to produce accurate answers based on document context.

  ğŸ“ Summarization (Map-Reduce)
     Splits large documents, summarizes each chunk, and then combines them into a final summary using a map-reduce approach.

  ğŸ—‚ Document Chunking
    Uses RecursiveCharacterTextSplitter to break PDFs into overlapping chunks for better context embedding and retrieval.

 ğŸ§ª Optional Parameters and Techniques
   k=5 in .as_retriever(k=5) retrieves the top 5 relevant chunks.

   max_new_tokens=256 or max_length=1000 limits the generation length.

  do_sample=False ensures deterministic outputs.

 allow_dangerous_deserialization=True is used for loading FAISS indexes from disk (be cautious).



This project demonstrates how to build a practical, local LLM-powered assistant for reading, querying, and summarizing legal documents. Itâ€™s fast, offline, and highly customizable. Ideal for both legal professionals and learners.
